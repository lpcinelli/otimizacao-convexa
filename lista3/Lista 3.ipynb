{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import buit-in modules\n",
    "\n",
    "import numpy as np\n",
    "import sympy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "%matplotlib notebook\n",
    "plt.rcParams.update({'font.size': 17})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import my own modules\n",
    "import sys\n",
    "sys.path.insert(0, '../utils')\n",
    "from mycvx import *\n",
    "from myplots import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q1 = sp.MatrixSymbol('Q1', 4, 4)\n",
    "Q2 = sp.MatrixSymbol('Q2', 4, 4)\n",
    "Q3 = sp.MatrixSymbol('Q3', 4, 4)\n",
    "Q4 = sp.MatrixSymbol('Q4', 4, 4)\n",
    "Q = sp.BlockMatrix([[Q1, Q2, Q3, Q4],\n",
    "                   [Q2, Q1, Q2, Q3],\n",
    "                   [Q3, Q2, Q1, Q2],\n",
    "                   [Q4, Q3, Q2, Q1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q1_num = sp.Matrix([[12, 8, 7, 6],\n",
    "                    [8, 12, 8, 7],\n",
    "                    [7, 8, 12, 8],\n",
    "                    [6, 7, 8, 12]])\n",
    "\n",
    "Q2_num = sp.Matrix([[3, 2, 1, 0],\n",
    "                    [2, 3, 2, 1],\n",
    "                    [1, 2, 3, 2],\n",
    "                    [0, 1, 2, 3]])\n",
    "\n",
    "Q3_num = sp.Matrix([[2, 1, 0, 0],\n",
    "                    [1, 2, 1, 0],\n",
    "                    [0, 1, 2, 1],\n",
    "                    [0, 0, 1, 2]])\n",
    "\n",
    "Q4_num = sp.eye(4)\n",
    "\n",
    "Q = Q.as_explicit()\n",
    "Q = Q.evalf(subs={Qk: Qk_num for Qk, Qk_num in zip([Q1,Q2,Q3,Q4],[Q1_num,Q2_num,Q3_num,Q4_num])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = -sp.Matrix([1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss1(x):\n",
    "    return (x.T*Q*x)/2 + b.T*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using 'Conjugate gradient' - algo 6.2\n",
      "\n",
      "x min [ 0.03423704  0.02423337  0.02423337  0.03423704 -0.00143237 -0.02088798\n",
      " -0.02088798 -0.00143237  0.03321913  0.02738606  0.02738606  0.03321913\n",
      " -0.00663057 -0.00486331 -0.00486331 -0.00663057]\n",
      "\n",
      "f(x min) [[-0.1190756]]\n",
      "\n",
      "nb evals {'nbFunEval': 9, 'nbGradEval': 9, 'nbHessEval': 9}\n"
     ]
    }
   ],
   "source": [
    "x0 = np.zeros((16,1))\n",
    "xHist = {}\n",
    "fxHist = {}\n",
    "nbEvalList = {}\n",
    "\n",
    "print('\\nUsing \\'Conjugate gradient\\' - algo 6.2')\n",
    "xHist, fxHist, nbEvalList = generalDescentMethod(x0, loss1, conjugateMethod, \\\n",
    "                                                eps=1e-6)\n",
    "print('\\nx min', xHist[-1])\n",
    "print('\\nf(x min)', fxHist[-1])\n",
    "print('\\nnb evals', nbEvalList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xmin= [ 1.00000007  1.00000011]\n",
      "f(xmin)= 3.43174783409e-14\n"
     ]
    }
   ],
   "source": [
    "def loss2(x):\n",
    "    return 100*(x[1] - x[0]**2)**2 + (1 - x[0])**2\n",
    "\n",
    "res = minimize(loss2, (0,0), method='nelder-mead', options={'xtol': 1e-6, 'disp': False})\n",
    "loss1Min = res.x\n",
    "print('xmin=',res.x)\n",
    "print('f(xmin)=',res.fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Fletcher-Reeves algorithm w/ line search params alpha=0.150, beta=0.500\n",
      "\n",
      "[-2.  2.]\n",
      "x min [ 1.00023612  1.0004733 ]\n",
      "f(x min) 5.58532165735e-08\n",
      "nb evals {'nbFunEval': 4035, 'nbGradEval': 407}\n",
      "\n",
      "[ 2. -2.]\n",
      "x min [ 1.00020202  1.00040557]\n",
      "f(x min) 4.10353033189e-08\n",
      "nb evals {'nbFunEval': 212, 'nbGradEval': 22}\n",
      "\n",
      "[-2. -2.]\n",
      "x min [ 1.00020931  1.00041952]\n",
      "f(x min) 4.38841004075e-08\n",
      "nb evals {'nbFunEval': 518, 'nbGradEval': 55}\n"
     ]
    }
   ],
   "source": [
    "x0s = np.array(((-2.0, 2.0), (2.0, -2.0), (-2.0, -2.0)))\n",
    "alpha=0.15\n",
    "beta=0.5\n",
    "\n",
    "print('Using Fletcher-Reeves algorithm w/ line search params alpha={:.3f}, beta={:.3f}'.format(alpha,beta))\n",
    "for x0 in x0s:\n",
    "    xHist = {}\n",
    "    fxHist = {}\n",
    "    nbEvalList = {}\n",
    "    print()\n",
    "    print(x0)\n",
    "    xHist, fxHist, nbEvalList = generalDescentMethod(x0, loss2, conjugateMethod, eps=1e-6,\n",
    "                                                     lineSearchMethod=backtrackingLineSearch,\n",
    "                                                     alpha=alpha,beta=beta)\n",
    "    print('x min', xHist[-1])\n",
    "    print('f(x min)', fxHist[-1])\n",
    "    print('nb evals',nbEvalList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xmin= [-16.29999878 -17.99999861]\n",
      "f(xmin)= -8.15\n"
     ]
    }
   ],
   "source": [
    "def loss3(x):\n",
    "    return 5*x[0]**2 - 9*x[0]*x[1] + 4.075*x[1]**2 + x[0]\n",
    "\n",
    "res = minimize(loss3, (0,0), method='nelder-mead', options={'xtol': 1e-6, 'disp': False})\n",
    "loss1Min = res.x\n",
    "print('xmin=',res.x)\n",
    "print('f(xmin)=',res.fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using conjugate gradient algorithm w/o line search\n",
      "x min [-16.3 -18. ]\n",
      "f(x min) -8.149999999999565\n",
      "nb evals {'nbGradEval': 3, 'nbHessEval': 3, 'nbFunEval': 3}\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array((1, 1))\n",
    "xHistCM = {}\n",
    "fxHistCM = {}\n",
    "nbEvalCMList = {}\n",
    "\n",
    "print('Using conjugate gradient algorithm w/o line search')\n",
    "\n",
    "xHistCM, fxHistCM, nbEvalCMList = generalDescentMethod(x0, loss3, conjugateMethod, eps=3e-7)\n",
    "print('x min', xHistCM[-1])\n",
    "print('f(x min)', fxHistCM[-1])\n",
    "print('nb evals',nbEvalCMList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Steepest Descent algorithm w/ line search params alpha=0.150, beta=0.500\n",
      "x min [-16.29989582 -17.99988454]\n",
      "f(x min) -8.14999999967\n",
      "nb evals {'nbFuncEval': 3607, 'nbGradEval': 1803}\n"
     ]
    }
   ],
   "source": [
    "xHistSD = {}\n",
    "fxHistSD = {}\n",
    "nbEvalSDList = {}\n",
    "\n",
    "alpha=0.15\n",
    "beta=0.5\n",
    "\n",
    "print('Using Steepest Descent algorithm w/ line search params alpha={:.3f}, beta={:.3f}'.format(alpha,beta))\n",
    "\n",
    "xHistSD, fxHistSD, nbEvalSDList = generalDescentMethod(x0, loss3, steepestDescent, eps=3e-7,\n",
    "                                                 lineSearchMethod=noLineSearch)\n",
    "#                                                  alpha=alpha,beta=beta)\n",
    "print('x min', xHistSD[-1])\n",
    "print('f(x min)', fxHistSD[-1])\n",
    "print('nb evals',nbEvalSDList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results on the cost function `loss3` after the first iterations for both algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results after 1 iter\n",
      "\n",
      "Conjugate gradient algorithm:\n",
      "x1 = [ 0.87651718  1.0524802 ]\n",
      "f(x1) = 0.9292130993762135\n",
      "\n",
      "Gradient descent algorithm:\n",
      "x1 = [ 0.87651718  1.0524802 ]\n",
      "f(x1) = 0.929213099376\n",
      "\n",
      "Results after 2 iter\n",
      "\n",
      "Conjugate gradient algorithm:\n",
      "x1 = [-16.3 -18. ]\n",
      "f(x1) = -8.149999999999597\n",
      "\n",
      "Gradient descent algorithm:\n",
      "x1 = [ 0.72660018  0.6997343 ]\n",
      "f(x1) = 0.785730135922\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,3):\n",
    "    print('\\nResults after', i, 'iter')\n",
    "    print('\\nConjugate gradient algorithm:')\n",
    "    print('x1 =',xHistCM[i])\n",
    "    print('f(x1) =',fxHistCM[i])\n",
    "\n",
    "    print('\\nGradient descent algorithm:')\n",
    "    print('x1 =',xHistSD[i])\n",
    "    print('f(x1) =',fxHistSD[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Quasi-Newton algorithm w/ line search params alpha=0.400, beta=0.900\n",
      "\n",
      "[-2.  2.]\n",
      "x min [ 1.00000005  1.00000011]\n",
      "f(x min) 4.91503231787e-15\n",
      "nb evals {'nbGradEval': 44, 'nbFunEval': 203}\n",
      "Using Quasi-Newton algorithm w/ line search params alpha=0.400, beta=0.900\n",
      "\n",
      "[ 2. -2.]\n",
      "x min [ 0.91766924  0.85337779]\n",
      "f(x min) 0.0194592640027\n",
      "nb evals {'nbGradEval': 20, 'nbFunEval': 440}\n",
      "Using Quasi-Newton algorithm w/ line search params alpha=0.400, beta=0.900\n",
      "\n",
      "[-2. -2.]\n",
      "x min [-0.58620359  0.40056381]\n",
      "f(x min) 2.84013482007\n",
      "nb evals {'nbGradEval': 36, 'nbFunEval': 510}\n"
     ]
    }
   ],
   "source": [
    "x0s = np.array(((-2.0, 2.0), (2.0, -2.0), (-2.0, -2.0)))\n",
    "xHist = {}\n",
    "fxHist = {}\n",
    "nbEvalList = {}\n",
    "\n",
    "alpha = 0.4\n",
    "beta = 0.9\n",
    "\n",
    "for x0 in x0s:\n",
    "    print('Using Quasi-Newton algorithm w/ line search params alpha={:.3f}, beta={:.3f}'.format(alpha,beta))\n",
    "\n",
    "    print()\n",
    "    print(x0)\n",
    "    xHist, fxHist, nbEvalList = generalDescentMethod(x0, loss2, quasiNewton, eps=3e-7,\n",
    "                                                           lineSearchMethod=backtrackingLineSearch,\n",
    "                                                           alpha=alpha, beta=beta,\n",
    "                                                           method=DFP)\n",
    "    print('x min', xHist[-1])\n",
    "    print('f(x min)', fxHist[-1])\n",
    "    print('nb evals',nbEvalList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Quasi-Newton algorithm w/ line search params alpha=0.100, beta=0.300\n",
      "x min [-16.3 -18. ]\n",
      "f(x min) -8.15\n",
      "nb evals {'nbGradEval': 6, 'nbFunEval': 14}\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array((0,0))\n",
    "xHist = {}\n",
    "fxHist = {}\n",
    "nbEvalList = {}\n",
    "\n",
    "alpha=0.1\n",
    "beta=0.3\n",
    "\n",
    "print('Using Quasi-Newton algorithm w/ line search params alpha={:.3f}, beta={:.3f}'.format(alpha,beta))\n",
    "\n",
    "xHist, fxHist, nbEvalList = generalDescentMethod(x0, loss3, quasiNewton, eps=3e-7,\n",
    "                                                       lineSearchMethod=backtrackingLineSearch,\n",
    "                                                       alpha=alpha, beta=beta,\n",
    "                                                       method=BFGS)\n",
    "print('x min', xHist[-1])\n",
    "print('f(x min)', fxHist[-1])\n",
    "print('nb evals',nbEvalList)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
