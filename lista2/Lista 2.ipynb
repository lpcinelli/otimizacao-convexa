{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import cvxpy as cvx\n",
    "from scipy.optimize import minimize\n",
    "import sympy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import sympy as sp\n",
    "from functools import reduce\n",
    "from mpl_toolkits import mplot3d\n",
    "from scipy.ndimage.interpolation import shift\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "# Implementar algoritmos 5.1, 5.2, algo do prob. 5.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ f(x) = (x_1^2 + x_2^2 - 1)^2 + (x_1 + x_2 -1)^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss1(x):\n",
    "    return (x[0]**2 + x[1]**2 -1)**2 + (x[0] + x[1] -1)**2\n",
    "\n",
    "nbDims = 2\n",
    "symbolVec = sp.symbols('a0:%d'%nbDims)\n",
    "cost = sp.lambdify(symbolVec, loss1(symbolVec), modules=['numpy'])\n",
    "gradCost = sp.lambdify(symbolVec,[sp.diff(loss1(symbolVec),var) for var in (symbolVec)],'numpy')\n",
    "hessianCost = sp.lambdify(symbolVec, sp.hessian(loss1(symbolVec),symbolVec),'numpy')\n",
    "\n",
    "domain = np.linspace(-10, 10,500)\n",
    "X, Y = np.meshgrid(domain, domain)\n",
    "Z = cost(X,Y)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(X,Y,Z, cmap=plt.cm.coolwarm)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "\n",
    "ax.set_title('$f(x)=%s$' % sp.latex(loss1(sp.symbols('x y'))));\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"lista2/relatorio/figs/loss4.eps\", \n",
    "# #                bbox_inches='tight', \n",
    "# #                transparent=True,\n",
    "#                pad_inches=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generalDescentMethod(x0, costSymbolic, specificDescentMethod, eps=1e-6, **args):\n",
    "    \n",
    "    symbolVec = sp.symbols('a0:%d'%len(x0))\n",
    "    descentAlgo = specificDescentMethod(symbolVec, costSymbolic, **args)\n",
    "    nbEvalDict = {}\n",
    "    params = {}\n",
    "    step = np.inf\n",
    "    xHist = [x0]\n",
    "    fxHist = [np.NaN]\n",
    "    \n",
    "    while np.linalg.norm(step) > eps:\n",
    "        \n",
    "        step, fx, partialNbEvalDict, params = descentAlgo(x0, params)\n",
    "        nbEvalDict = { k: nbEvalDict.get(k, 0) + partialNbEvalDict.get(k, 0) for k in set(partialNbEvalDict) }\n",
    "        x0 += step\n",
    "        xHist.append(x0)\n",
    "        fxHist.append(fx)\n",
    "\n",
    "    return xHist, fxHist, nbEvalDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def steepestDescent(symbolVec, costSymbolic, **args):\n",
    "\n",
    "    cost = sp.lambdify(symbolVec, costSymbolic(symbolVec), modules=['numpy'])\n",
    "    gradCost = sp.lambdify(symbolVec,[sp.diff(costSymbolic(symbolVec),var) for var in (symbolVec)],'numpy')\n",
    "    hessianCost = sp.lambdify(symbolVec, sp.hessian(costSymbolic(symbolVec),symbolVec),'numpy')\n",
    "    lineSearchMethod = args.pop('lineSearchMethod', noLineSearch)\n",
    "\n",
    "    def steepestDescentAlgo(x0, params):\n",
    "\n",
    "        dfx = np.asarray(gradCost(*x0))\n",
    "        direction = -dfx\n",
    "        t,f, nbEval, newParams = lineSearchMethod(x0, dfx, direction, cost, params, **args)\n",
    "        step = t*direction\n",
    "\n",
    "        return step, f, {'nbFuncEval': nbEval, 'nbGradEval':1}, newParams\n",
    "\n",
    "    return steepestDescentAlgo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def newtonDescent(symbolVec, costSymbolic, **args):\n",
    "\n",
    "    cost = sp.lambdify(symbolVec, costSymbolic(symbolVec), modules=['numpy'])\n",
    "    gradCost = sp.lambdify(symbolVec,[sp.diff(costSymbolic(symbolVec),var) for var in (symbolVec)],'numpy')\n",
    "    hessianCost = sp.lambdify(symbolVec, sp.hessian(costSymbolic(symbolVec),symbolVec),'numpy')\n",
    "    lineSearchMethod = args.pop('lineSearchMethod', noLineSearch)\n",
    "    beta = args.pop('beta', 0.3)\n",
    "\n",
    "    def basicNewtonAlgo(x0, params):\n",
    "\n",
    "        dfx = np.asarray(gradCost(*x0))\n",
    "        Hx = np.asarray(hessianCost(*x0))\n",
    "        eigVals = np.linalg.eigvals(Hx)\n",
    "        minVal = np.min(eigVals)\n",
    "        if np.any(eigVals < 0.1):\n",
    "            beta = 1.5*np.abs(np.min(eigVals))\n",
    "            Hx = (Hx + beta*np.eye(Hx.shape[0]))/(1+beta)\n",
    "\n",
    "        HxInv = np.linalg.inv(Hx)\n",
    "        direction = -HxInv.dot(dfx)        \n",
    "        t,f, nbEval, newParams = lineSearchMethod(x0, dfx, direction, cost, params, **args)\n",
    "        step = t*direction\n",
    "\n",
    "        return step, f, {'nbFunEval': nbEval, 'nbGradEval':1, 'nbHessEval':1}, newParams\n",
    "\n",
    "    return basicNewtonAlgo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gaussNewtonDescent(symbolVec, costSymbolic, **args):\n",
    "\n",
    "    cost = sp.lambdify(symbolVec, loss2Vec(symbolVec), modules=['numpy'])\n",
    "    scalarCost = sp.lambdify(symbolVec,loss2Vec(symbolVec).dot(loss2Vec(symbolVec)), modules=['numpy'])\n",
    "#     gradCost = sp.lambdify(symbolVec,[sp.diff(loss2Vec(symbolVec),var) for var in (symbolVec)],'numpy')\n",
    "    jacobian = sp.lambdify(symbolVec, loss2Vec(symbolVec).jacobian(symbolVec), modules=['numpy'])\n",
    "    lineSearchMethod = args.pop('lineSearchMethod', noLineSearch)\n",
    "    beta = args.pop('beta', 0.3)\n",
    "\n",
    "    def gaussNewtonAlgo(x0, params):\n",
    "        \n",
    "        \n",
    "        fx = cost(*(x0))\n",
    "        Jx = np.asarray(jacobian(*x0))\n",
    "        dfx = 2*Jx.T.dot(fx)\n",
    "        Hx = 2*Jx.T.dot(Jx)\n",
    "        \n",
    "        L,D = hessianCorrectionMatDav(Hx)\n",
    "        y = - L.dot(dfx)\n",
    "        direction = L.T.dot(np.linalg.inv(D).dot(y))[:,0]\n",
    "\n",
    "        t,f, nbEval, newParams = lineSearchMethod(x0, dfx, direction, scalarCost, params, **args)\n",
    "        step = t*direction\n",
    "\n",
    "        return step, cost(*(x0+step)), {'nbFunEval': nbEval, 'nbGradEval': dfx.shape[0]}, newParams\n",
    "\n",
    "    return gaussNewtonAlgo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessianCorrectionMatDav(H):\n",
    "    n = H.shape[1]\n",
    "    L = np.zeros(H.shape)\n",
    "    D = np.zeros(n)\n",
    "    h00 = 0\n",
    "    \n",
    "    if H[0,0] > 0:\n",
    "        h00 = H[0,0]\n",
    "    else:\n",
    "        h00 = 1\n",
    "    \n",
    "    for k in range(1,n):\n",
    "        m = k - 1\n",
    "        L[m,m] = 1\n",
    "        if H[m,m] <= 0:\n",
    "            H[m,m] = h00\n",
    "        \n",
    "        for i in range(k,n):\n",
    "            L[i,m] = - H[i,m]/H[m,m]\n",
    "            H[i,m] = 0\n",
    "            for j in range(k,n):\n",
    "                H[i,j] += L[i,m]*H[m,j]\n",
    "        \n",
    "        if H[k,k] > 0 and H[k,k] < h00:\n",
    "            h00 = H[k,k]\n",
    "        \n",
    "    L[-1,-1] = 1\n",
    "    if H[-1,-1] <= 0:\n",
    "        H[-1,-1] = h00\n",
    "    for i in range(n):\n",
    "        D[i] = H[i,i]\n",
    "\n",
    "    return L, np.diag(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def noLineSearch(x, dfx, vec, cost, params, **args):\n",
    "\n",
    "    alphaHat = params.pop('alpha', 1)\n",
    "    fxk = params.pop('fxk', None)\n",
    "    nbEval = 0\n",
    "    if fxk is None:\n",
    "        fxk = cost(*x)\n",
    "        nbEval += 1\n",
    "    \n",
    "    fHat = cost(*(x + alphaHat*vec))\n",
    "    dfx2alpha = np.inner(dfx,dfx)*alphaHat\n",
    "    alpha = (dfx2alpha*alphaHat)/(2*(fHat - fxk + dfx2alpha))\n",
    "    fx = cost(*(x + alpha*vec))\n",
    "    \n",
    "    return alpha, fx, nbEval+2, {'alpha': alpha, 'fxk': fx}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backtrackingLineSearch(x, dfx, vec, cost, params, **args):\n",
    "    \n",
    "    domain = args.pop('domain', None)\n",
    "    fx = args.pop('fx', None)\n",
    "    alpha = args.pop('alpha', 0.15)\n",
    "    beta = args.pop('beta', 0.5)\n",
    "    \n",
    "    t = 1\n",
    "    nbEval = 1\n",
    "    \n",
    "    if fx is None:\n",
    "        fx = cost(*x)\n",
    "        nbEval += 1\n",
    "    \n",
    "    if domain is not None:\n",
    "        while (x + t*vec <= domain[0]) or (domain[1] <= x + t*vec):\n",
    "            ptiny\n",
    "            t *= beta\n",
    "\n",
    "    f = cost(*(x + t*vec))\n",
    "    \n",
    "    while f > (fx + alpha*t*np.vdot(dfx,vec)):\n",
    "        t *= beta\n",
    "        f = cost(*(x + t*vec))\n",
    "        nbEval += 1\n",
    "\n",
    "    return t,f, nbEval, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0s = np.array(((4.0,4.0), (4.0,-4.0), (-4.0,4.0), (-4.0,-4.0)))\n",
    "xHist = {}\n",
    "fxHist = {}\n",
    "nbEvalList = {}\n",
    "\n",
    "print('Using algo 5.1')\n",
    "for x0 in x0s:\n",
    "    print()\n",
    "    print(x0)\n",
    "    xHist, fxHist, nbEvalList = generalDescentMethod(x0, loss1, steepestDescent,\n",
    "                                                     lineSearchMethod=backtrackingLineSearch,\n",
    "                                                     alpha=0.01,beta=0.3)\n",
    "    print('x min', xHist[-1])\n",
    "    print('f(x min)', fxHist[-1])\n",
    "    print('nb evals',nbEvalList)\n",
    "    \n",
    "x0s = np.array(((4.0,4.0), (4.0,-4.0), (-4.0,4.0), (-4.0,-4.0)))\n",
    "xHist = {}\n",
    "fxHist = {}\n",
    "nbEvalList = {}\n",
    "\n",
    "print('Using algo 5.2')\n",
    "for x0 in x0s:\n",
    "    print()\n",
    "    print(x0)\n",
    "    xHist, fxHist, nbEvalList = generalDescentMethod(x0, loss1, steepestDescent,\n",
    "                                                          lineSearchMethod=noLineSearch)\n",
    "    print('x min', xHist[-1])\n",
    "    print('f(x min)', fxHist[-1])\n",
    "    print('nb evals',nbEvalList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x0s = np.array(((4.0,4.0), (4.0,-4.0), (-4.0,4.0), (-4.0,-4.0)))\n",
    "xHist = {}\n",
    "fxHist = {}\n",
    "nbEvalList = {}\n",
    "\n",
    "print('Using algo 5.2')\n",
    "for x0 in x0s:\n",
    "    print()\n",
    "    print(x0)\n",
    "    xHist, fxHist, nbEvalList = generalDescentMethod(x0, loss1, steepestDescent,\n",
    "                                                          lineSearchMethod=noLineSearch)\n",
    "    print('x min', xHist[-1])\n",
    "    print('f(x min)', fxHist[-1])\n",
    "    print('nb evals',nbEvalList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0s = np.array(((4.0,4.0), (4.0,-4.0), (-4.0,4.0), (-4.0,-4.0)))\n",
    "xHist = {}\n",
    "fxHist = {}\n",
    "nbEvalList = {}\n",
    "\n",
    "print('Using Hessian modified algo 5.3')\n",
    "for x0 in x0s:\n",
    "    print()\n",
    "    print(x0)\n",
    "    xHist, fxHist, nbEvalList = generalDescentMethod(x0, loss1, newtonDescent,\n",
    "                                                          lineSearchMethod=backtrackingLineSearch)\n",
    "    print('x min', xHist[-1])\n",
    "    print('f(x min)', fxHist[-1])\n",
    "    print('nb evals',nbEvalList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss2(x):\n",
    "    return (x[0] + 10*x[1])**2 + 5*(x[2] - x[3])**2 + (x[1] - 2*x[2])**4 + 100*(x[0] - x[3])**4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss2Vec(x):\n",
    "    return sp.Matrix(((x[0] + 10*x[1]), (sp.sqrt(5)*(x[2] - x[3])), \n",
    "                    ((x[1] - 2*x[2])**2), (10*(x[0] - x[3])**2)))\n",
    "\n",
    "nbDims = 4\n",
    "symbolVec = sp.symbols('a0:%d'%nbDims)\n",
    "cost = sp.lambdify(symbolVec, loss2Vec(symbolVec), modules=['numpy'])\n",
    "gradCost = sp.lambdify(symbolVec,[sp.diff(loss2Vec(symbolVec),var) for var in (symbolVec)],'numpy')\n",
    "\n",
    "jacobian = sp.lambdify(symbolVec, loss2Vec(symbolVec).jacobian(symbolVec), modules=['numpy'])\n",
    "print(loss2Vec(symbolVec))\n",
    "jacobian(*x0).T.dot(jacobian(*x0))\n",
    "cost(*x0)\n",
    "(loss2Vec(symbolVec)).dot(loss2Vec(symbolVec))\n",
    "scalarCost = sp.lambdify(symbolVec,loss2Vec(symbolVec).dot(loss2Vec(symbolVec)), modules=['numpy'])\n",
    "scalarCost(*x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Steepest descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x0s = np.array(((-2.0, -1.0, 1.0, 2.0), (200, -200, 100, -100)))\n",
    "xHist = {}\n",
    "fxHist = {}\n",
    "nbEvalList = {}\n",
    "\n",
    "print('Using algo 5.2 w/ line search w/ params alpha=0.15,beta=0.5')\n",
    "for x0 in x0s:\n",
    "    print()\n",
    "    print(x0)\n",
    "    xHist, fxHist, nbEvalList = generalDescentMethod(x0, loss2, steepestDescent, eps=1e-6,\n",
    "                                                          lineSearchMethod=backtrackingLineSearch)\n",
    "#                                                      alpha=0.01,beta=0.3)\n",
    "    print('x min', xHist[-1])\n",
    "    print('f(x min)', fxHist[-1])\n",
    "    print('nb evals',nbEvalList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x0s = np.array(((-2.0, -1.0, 1.0, 2.0), (200, -200, 100, -100)))\n",
    "xHist = {}\n",
    "fxHist = {}\n",
    "nbEvalList = {}\n",
    "  \n",
    "print('\\n\\nUsing algo 5.2 w/ line search w/ params alpha=0.01,beta=0.3')\n",
    "for x0 in x0s:\n",
    "    print()\n",
    "    print(x0)\n",
    "    xHist, fxHist, nbEvalList = generalDescentMethod(x0, loss2, steepestDescent, eps=1e-6,\n",
    "                                                          lineSearchMethod=backtrackingLineSearch,\n",
    "                                                     alpha=0.01,beta=0.3)\n",
    "    print('x min', xHist[-1])\n",
    "    print('f(x min)', fxHist[-1])\n",
    "    print('nb evals',nbEvalList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x0s = np.array(((-2.0, -1.0, 1.0, 2.0), (200, -200, 100, -100)))\n",
    "xHist = {}\n",
    "fxHist = {}\n",
    "nbEvalList = {}\n",
    "\n",
    "print('\\n\\nUsing algo 5.2 w/o line search')\n",
    "for x0 in x0s:\n",
    "    print()\n",
    "    print(x0)\n",
    "    xHist, fxHist, nbEvalList = generalDescentMethod(x0, loss2, steepestDescent, eps=1e-6,\n",
    "                                                          lineSearchMethod=noLineSearch)\n",
    "    print('x min', xHist[-1])\n",
    "    print('f(x min)', fxHist[-1])\n",
    "    print('nb evals',nbEvalList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Newton "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0s = np.array(((-2.0, -1.0, 1.0, 2.0), (200, -200, 100, -100)))\n",
    "xHist = {}\n",
    "fxHist = {}\n",
    "nbEvalList = {}\n",
    "\n",
    "print('Using algo 5.3 w/ line search w/ params alpha=0.15,beta=0.5')\n",
    "for x0 in x0s:\n",
    "    print()\n",
    "    print(x0)\n",
    "    xHist, fxHist, nbEvalList = generalDescentMethod(x0, loss2, newtonDescent, eps=1e-6,\n",
    "                                                          lineSearchMethod=backtrackingLineSearch)\n",
    "    print('x min', xHist[-1])\n",
    "    print('f(x min)', fxHist[-1])\n",
    "    print('nb evals',nbEvalList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using algo 5.3 w/ line search w/ params alpha=0.01,beta=0.3\n",
      "\n",
      "[-2. -1.  1.  2.]\n",
      "x min [  4.11797603e-06  -4.11797603e-07   2.89051310e-06   2.89051310e-06]\n",
      "f(x min) 1.69780832051e-21\n",
      "nb evals {'nbHessEval': 58, 'nbFunEval': 116, 'nbGradEval': 58}\n",
      "\n",
      "[ 200. -200.  100. -100.]\n",
      "x min [  4.01913816e-06  -4.01913816e-07   2.82113727e-06   2.82113727e-06]\n",
      "f(x min) 1.54058424005e-21\n",
      "nb evals {'nbHessEval': 87, 'nbFunEval': 174, 'nbGradEval': 87}\n"
     ]
    }
   ],
   "source": [
    "x0s = np.array(((-2.0, -1.0, 1.0, 2.0), (200, -200, 100, -100)))\n",
    "xHist = {}\n",
    "fxHist = {}\n",
    "nbEvalList = {}\n",
    "   \n",
    "print('Using algo 5.3 w/ line search w/ params alpha=0.01,beta=0.3')\n",
    "for x0 in x0s:\n",
    "    xHist = {}\n",
    "    fxHist = {}\n",
    "    nbEvalList = {}\n",
    "    print()\n",
    "    print(x0)\n",
    "    xHist, fxHist, nbEvalList = generalDescentMethod(x0, loss2, newtonDescent, eps=1e-6,\n",
    "                                                     lineSearchMethod=backtrackingLineSearch,\n",
    "                                                     alpha=0.01,beta=0.3)\n",
    "    print('x min', xHist[-1])\n",
    "    print('f(x min)', fxHist[-1])\n",
    "    print('nb evals',nbEvalList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using algo 5.3 w/o line search\n",
      "\n",
      "[-2. -1.  1.  2.]\n",
      "x min [-1.56134028 -0.190389    0.95775448  1.24652367]\n",
      "f(x min) 6247.99546717\n",
      "nb evals {'nbHessEval': 20, 'nbFunEval': 41, 'nbGradEval': 20}\n",
      "\n",
      "[ 200. -200.  100. -100.]\n",
      "x min [ 254.51510172  -77.43336704  101.67907809   43.9214593 ]\n",
      "f(x min) 202906075604.0\n",
      "nb evals {'nbHessEval': 27, 'nbFunEval': 55, 'nbGradEval': 27}\n"
     ]
    }
   ],
   "source": [
    "x0s = np.array(((-2.0, -1.0, 1.0, 2.0), (200, -200, 100, -100)))\n",
    "xHist = {}\n",
    "fxHist = {}\n",
    "nbEvalList = {}\n",
    "     \n",
    "print('Using algo 5.3 w/o line search')\n",
    "for x0 in x0s:\n",
    "    xHist = {}\n",
    "    fxHist = {}\n",
    "    nbEvalList = {}\n",
    "    print()\n",
    "    print(x0)\n",
    "    xHist, fxHist, nbEvalList = generalDescentMethod(x0, loss2, newtonDescent, eps=1e-6,\n",
    "                                                          lineSearchMethod=noLineSearch)\n",
    "    print('x min', xHist[-1])\n",
    "    print('f(x min)', fxHist[-1])\n",
    "    print('nb evals',nbEvalList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por que ambos algoritmos (Steepest descent e Newton) quando usam o método sem (backtracking) line search falham miseravelmente na busca pelo mínimo global $[0, 0, 0, 0]$ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Gauss-Newton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using algo 5.5 w/ line search w/ params alpha=0.01,beta=0.3\n",
      "x min [-0.00451587  0.00045088 -0.00569479 -0.0056964 ]\n",
      "f(x min) [[ -7.09322008e-06]\n",
      " [  3.60180484e-06]\n",
      " [  1.40196630e-04]\n",
      " [  1.39366976e-05]]\n",
      "nb evals {'nbFunEval': 264023, 'nbGradEval': 120500}\n",
      "x min [ 0.00452268 -0.00045156  0.00570336  0.00570498]\n",
      "f(x min) [[  7.12830423e-06]\n",
      " [ -3.61811352e-06]\n",
      " [  1.40618787e-04]\n",
      " [  1.39783579e-05]]\n",
      "nb evals {'nbFunEval': 262589, 'nbGradEval': 119960}\n"
     ]
    }
   ],
   "source": [
    "x0s = np.array(((-2.0, -1.0, 1.0, 2.0), (200, -200, 100, -100)))\n",
    "xHist = {}\n",
    "fxHist = {}\n",
    "nbEvalList = {}\n",
    "   \n",
    "print('Using algo 5.5 w/ line search w/ params alpha=0.01,beta=0.3')\n",
    "for x0 in x0s:\n",
    "    xHist = {}\n",
    "    fxHist = {}\n",
    "    nbEvalList = {}\n",
    "#     print()\n",
    "#     print(x0)\n",
    "    xHist, fxHist, nbEvalList = generalDescentMethod(x0, loss2, gaussNewtonDescent, eps=1e-6,\n",
    "                                                     lineSearchMethod=backtrackingLineSearch,\n",
    "                                                     alpha=0.01,beta=0.3)\n",
    "    print('x min', xHist[-1])\n",
    "    print('f(x min)', fxHist[-1])\n",
    "    print('nb evals',nbEvalList)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
